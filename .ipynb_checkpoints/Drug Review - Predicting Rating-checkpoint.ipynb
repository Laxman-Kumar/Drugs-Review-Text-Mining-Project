{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is used to solve the regression probelm of our dataset. The problem statement focuses on predicting the 10 scale rating score using the rest of the attributes\n",
    "\n",
    "### We will buliding traditional machine learning model as well as state of the art neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Student Name : Laxman Kumar <br> Subject Name : IST 736 Text Mining <br> University : Syracuse University <br> Professor : Prof Yingya Li</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resouces used are https://www.kaggle.com/jessicali9530/kuc-hackathon-winter-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Traditional Machine Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.svm import LinearSVC,SVC,SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized function for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,normalize=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['rating'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(binary=False, min_df = 20, stop_words = stop_words, analyzer = 'word',ngram_range=(1,2))\n",
    "countVector_train = countVectorizer.fit_transform(X_train)\n",
    "countVector_val = countVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(min_df = 20, stop_words = stop_words, analyzer = 'word',ngram_range=(1,2))\n",
    "tfidfVector_train = tfidfVectorizer.fit_transform(X_train)\n",
    "tfidfVector_train = tfidfVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and comparing different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized function for calculating the accuracy of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print(\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)\n",
    "    return accuracy, train_test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for generating report and important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateReport(Ytest,y_pred,labels):\n",
    "    print(\"--------CONFUSION MATRIX-----------\")\n",
    "    print(confusion_matrix(Ytest, y_pred, labels=labels))\n",
    "    print()\n",
    "    print(\"---------EVALUATION REPORT----------\")\n",
    "    print(classification_report(Ytest, y_pred))\n",
    "\n",
    "def show_most_and_least_informative_features(vectorizer, clf, class_idx=0, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[class_idx], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[-n:])\n",
    "    \n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_2, fn_2, coef_1, fn_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating report for three models with their default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Linear SVC\n"
     ]
    }
   ],
   "source": [
    "names = [\"Linear SVC\", \"Multinomial NB\", \"Bernoulli NB\"]\n",
    "classifiers = [LinearSVC(),MultinomialNB(),BernoulliNB()]\n",
    "\n",
    "zipped_clf = zip(names,classifiers)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "tvec1 = CountVectorizer()\n",
    "def classifier_comparator(vectorizer=tvec1,  stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, ngram_range=ngram_range)\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([('vectorizer', vectorizer),('classifier', c)])\n",
    "        print(\"Validation result for {}\".format(n))\n",
    "        clf_accuracy,tt_time = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n",
    "        result.append((n,clf_accuracy,tt_time))\n",
    "        #print(show_most_and_least_informative_features(tvec1,nb))\n",
    "    return result\n",
    "\n",
    "#trigram_result = classifier_comparator(ngram_range=(1,1))\n",
    "#trigram_result = classifier_comparator(ngram_range=(1,2))\n",
    "trigram_result = classifier_comparator(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running several combination of vectorizer and ngram range Count vectorizer with ngram range of 1-3 gives the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the best model with different combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(binary=False,  stop_words = stop_words, analyzer = 'word',ngram_range=(1,3))\n",
    "countVector_train = countVectorizer.fit_transform(X_train)\n",
    "countVector_val = countVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Best and least features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(countVector_train, y_train)\n",
    "y_pred_class = mnb.predict(countVector_val)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_and_least_informative_features(countVectorizer,mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB Best and least features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(countVector_train, y_train)\n",
    "y_pred_class = bnb.predict(countVector_val)\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_and_least_informative_features(countVectorizer,bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = LinearSVC(C=1.5,max_iter=1500)\n",
    "nb.fit(countVector_train, y_train)\n",
    "y_pred_class = nb.predict(countVector_val)\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_and_least_informative_features(countVectorizer,nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['1','2','3','4','5','6','7','8','9','10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred=y_pred_class, y_true=y_test)\n",
    "plot_confusion_matrix(cm           = cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = [\"class 1\", \"class 2\", \"class 3\", \"class 4\", \"class 5\", \n",
    "                                                        \"class 6\",\"class 7\",\"class 8\",\"class 9\",\"class 10\"],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df['review'], df['rating'], test_size=0.2, random_state=42)\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_valid, y_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "x_train_vectors = vectorizer.fit_transform(x_train)\n",
    "x_valid_vectors = vectorizer.transform(x_valid)\n",
    "x_test_vectors = vectorizer.transform(x_test)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=min(1000, x_train_vectors.shape[1]))\n",
    "selector.fit(x_train_vectors, y_train)\n",
    "\n",
    "x_train_vectors = selector.transform(x_train_vectors).astype('float32')\n",
    "x_valid_vectors = selector.transform(x_valid_vectors).astype('float32')\n",
    "x_test_vectors = selector.transform(x_test_vectors).astype('float32')\n",
    "\n",
    "new_x_train = np.array(x_train_vectors)\n",
    "new_x_valid = np.array(x_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_valid_one_hot = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp3 = keras.models.Sequential()\n",
    "\n",
    "model_mlp3.add(keras.layers.Dense(1024, input_shape=(1000,)))\n",
    "model_mlp3.add(keras.layers.Activation('relu'))\n",
    "\n",
    "model_mlp3.add(keras.layers.Dense(512))\n",
    "model_mlp3.add(keras.layers.Activation('relu'))\n",
    "\n",
    "model_mlp3.add(keras.layers.Dense(256, activation='relu'))\n",
    "model_mlp3.add(keras.layers.Dense(11, activation='sigmoid'))\n",
    "\n",
    "model_mlp3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_mlp3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural net using train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model_mlp3.fit(x_train_vectors, y_train_one_hot, epochs=25, batch_size=64, validation_data=(x_valid_vectors, y_valid_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting the accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history3.history['accuracy'])\n",
    "plt.plot(history3.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2_3 = model_mlp3.predict_classes(x_test_vectors)\n",
    "print(classification_report(y_test, y_pred2_3, target_names=[\"class 1\", \"class 2\", \"class 3\", \"class 4\", \"class 5\", \"class 6\",\"class 7\",\"class 8\",\"class 9\",\"class 10\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2_3 = confusion_matrix(y_pred=y_pred2_3, y_true=y_test)\n",
    "\n",
    "plot_confusion_matrix(cm           = cm2_3, \n",
    "                      normalize    = False,\n",
    "                      target_names = [\"class 1\", \"class 2\", \"class 3\", \"class 4\", \"class 5\", \n",
    "                                                        \"class 6\",\"class 7\",\"class 8\",\"class 9\",\"class 10\"],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
